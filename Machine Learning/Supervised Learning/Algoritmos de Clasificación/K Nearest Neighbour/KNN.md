# **K-Nearest Neighbors (KNN)**

**K-Nearest Neighbors (KNN)** es un algoritmo de **clasificaci√≥n supervisada** que predice la clase de un punto en base a las clases de sus **K vecinos m√°s cercanos** en el espacio de caracter√≠sticas. Es uno de los algoritmos m√°s sencillos y vers√°tiles, utilizado tanto para **clasificaci√≥n** como para **regresi√≥n**.

---

## üîë **¬øC√≥mo Funciona KNN?**

1. **Definir K**: 
   - Se selecciona el par√°metro **K**, que representa la cantidad de vecinos m√°s cercanos que se usar√°n para clasificar un nuevo punto. Ejemplo: \( K = 9 \).

2. **C√°lculo de la Distancia**: 
   - Para predecir la clase de un nuevo punto, calculamos la distancia entre este punto y los puntos de entrenamiento. Las distancias m√°s comunes son:
   
   - **Distancia Euclidiana** (usada generalmente para datos continuos):
     \[
     d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
     \]
   - **Distancia Manhattan** (cuando las caracter√≠sticas son discretas):
     \[
     d(x, y) = \sum_{i=1}^{n} |x_i - y_i|
     \]
   
   - Donde \( x \) y \( y \) son dos puntos en el espacio de caracter√≠sticas, y \(n\) es el n√∫mero de caracter√≠sticas.

3. **Seleccionar los K Vecinos M√°s Cercanos**: 
   - Ordenamos los puntos de entrenamiento seg√∫n su distancia al punto nuevo y seleccionamos los **K puntos m√°s cercanos**.

4. **Votaci√≥n Mayoritaria**: 
   - El nuevo punto se clasifica seg√∫n la clase **mayor√≠a** entre los \(K\) vecinos m√°s cercanos. En caso de empate, se pueden usar m√©todos adicionales (como la distancia ponderada).

---

## üìä **F√≥rmula para Predicci√≥n de Clase:**

La predicci√≥n de la clase \( \hat{y} \) para un nuevo punto \( x \) es:

\[
\hat{y} = \text{modo}(y_1, y_2, \dots, y_K)
\]

Donde:
- \( y_1, y_2, \dots, y_K \) son las clases de los \(K\) vecinos m√°s cercanos.
- **Modo** significa la clase que m√°s veces aparece entre ellos.

---

## üèÜ **Ventajas de KNN**:
- **Simplicidad**: Muy f√°cil de entender e implementar.
- **No param√©trico**: No hace suposiciones sobre la distribuci√≥n de los datos, lo que lo hace √∫til en muchos escenarios.
- **Flexibilidad**: Puede ser utilizado tanto para **clasificaci√≥n** como para **regresi√≥n**.
- **Modelo intuitivo**: F√°cil de explicar y de visualizar.

---

## ‚ö†Ô∏è **Desventajas de KNN**:
- **Computacionalmente costoso**: El c√°lculo de distancias para cada punto de datos es lento, especialmente para grandes vol√∫menes de datos.
- **Sensibilidad a la escala**: Si las caracter√≠sticas tienen diferentes escalas, la distancia puede verse distorsionada. Es necesario **escalar** los datos.
- **Eficiencia de memoria**: El algoritmo requiere almacenar todo el conjunto de entrenamiento, lo que puede ser ineficiente en t√©rminos de memoria.
- **Sensibilidad al ruido**: KNN puede verse afectado por caracter√≠sticas irrelevantes o ruido en los datos.

---

## üí° **Consideraciones Importantes**:
- **Selecci√≥n de K**: Elegir el valor correcto de \(K\) es crucial. Un \(K\) peque√±o puede ser sensible al ruido, mientras que un \(K\) grande puede suavizar demasiado las fronteras de decisi√≥n.
- **Escalado de los datos**: Para que las distancias sean significativas, es importante **escalar o normalizar** las caracter√≠sticas antes de aplicar el algoritmo (por ejemplo, utilizando `StandardScaler` o `MinMaxScaler`).
- **Votaci√≥n ponderada**: A veces, es √∫til ponderar las clases de los vecinos por su **distancia inversa**, dando m√°s peso a los puntos m√°s cercanos.

---

## üìè **Resumen Matem√°tico de KNN**:

1. Para cada punto \( x \) a clasificar, calculamos la distancia \( d(x, x_i) \) entre \( x \) y todos los puntos \( x_i \) del conjunto de entrenamiento:
   \[
   d(x, x_i) = \sqrt{\sum_{i=1}^{n} (x_i - x)^2}
   \]

2. Seleccionamos los \( K \) vecinos m√°s cercanos y realizamos la **votaci√≥n mayoritaria** para determinar la clase de \( x \).

---

## üìà **¬øPara Qu√© Se Usa KNN?**

- **Clasificaci√≥n de texto**: Como la clasificaci√≥n de correos electr√≥nicos como "spam" o "no spam".
- **Reconocimiento de im√°genes**: Clasificaci√≥n de im√°genes basadas en similitudes de caracter√≠sticas.
- **Sistemas de recomendaci√≥n**: Encontrar productos similares a los preferidos por un usuario.
- **An√°lisis de anomal√≠as**: Identificar outliers o puntos de datos at√≠picos.

---

## üßë‚Äçüíª **Ejemplo Visual con KNN**

![image](https://github.com/user-attachments/assets/e714fa67-d310-48ae-ae99-89b85ee7a449)


---

## üìâ **Conclusi√≥n**:

**K-Nearest Neighbors (KNN)** es uno de los algoritmos m√°s sencillos pero poderosos para clasificaci√≥n y regresi√≥n. Su eficiencia depende del tama√±o del conjunto de datos y de la elecci√≥n de **K**. Aunque es simple y muy intuitivo, requiere una buena selecci√≥n de par√°metros y un preprocesamiento adecuado (escalado de datos) para obtener buenos resultados.
