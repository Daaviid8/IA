# ü™¥ √Årbol de Decisi√≥n: Explicaci√≥n Clara y Rigurosa

## üìå ¬øQu√© es un √Årbol de Decisi√≥n?

Un **√Årbol de Decisi√≥n** es un modelo de **aprendizaje supervisado** utilizado para **clasificaci√≥n** y **regresi√≥n**. Su objetivo es **dividir los datos en grupos** m√°s homog√©neos bas√°ndose en las caracter√≠sticas del conjunto de datos, hasta lograr una clasificaci√≥n (en el caso de clasificaci√≥n) o una predicci√≥n (en el caso de regresi√≥n).

### Estructura del √Årbol de Decisi√≥n

Imagina un √°rbol invertido (con la ra√≠z en la parte superior). Cada **nodo** del √°rbol representa una **decisi√≥n** basada en una caracter√≠stica del conjunto de datos. Los **nodos hoja** representan las **predicciones finales**.

### Ejemplo Sencillo: Clasificaci√≥n de Frutas

Sup√≥n que queremos clasificar una fruta como **manzana** o **naranja** en funci√≥n de dos caracter√≠sticas: **color** y **peso**.

1. En el **primer nodo**, decidimos si el color de la fruta es **rojo** o **no rojo**:
   - Si es **rojo**, la fruta es una **manzana**.
   - Si es **no rojo**, pasamos al siguiente nodo.

2. En el **segundo nodo**, evaluamos si el **peso** es mayor o menor a un umbral (150 gramos):
   - Si es **menor de 150 gramos**, la fruta es una **manzana**.
   - Si es **mayor de 150 gramos**, la fruta es una **naranja**.

---

## üî¢ ¬øC√≥mo Funciona Matem√°ticamente un √Årbol de Decisi√≥n?

El √°rbol se construye **dividiendo el espacio de las caracter√≠sticas** de tal forma que los datos en cada regi√≥n resultante sean lo m√°s homog√©neos posible en cuanto a la variable objetivo.

### 1. **Criterios de Divisi√≥n**

El algoritmo decide qu√© caracter√≠stica usar para dividir los datos bas√°ndose en un **criterio de impureza**. Los m√°s comunes son:

- **√çndice de Gini**: Mide la "impureza" de un nodo. El valor de Gini es 0 cuando todos los elementos pertenecen a la misma clase (perfectamente puro), y el valor es 1 cuando las clases est√°n igualmente distribuidas.
  
  F√≥rmula del √çndice de Gini para una clase:
  Gini(S)=1‚àí 
i=1
‚àë
k
‚Äã
 p 
i
2
‚Äã

  Donde:
  - \( p_i \) es la proporci√≥n de elementos de la clase \( i \) dentro de un nodo.
  - \( k \) es el n√∫mero total de clases.

- **Entrop√≠a**: Mide la incertidumbre o el desorden de un conjunto de datos. La entrop√≠a es m√≠nima cuando todos los elementos de un nodo pertenecen a la misma clase.

  F√≥rmula de la entrop√≠a:
  \[
  Entrop√≠a(S) = - \sum_{i=1}^{k} p_i \log_2(p_i)
  \]
  Donde:
  - \( p_i \) es la proporci√≥n de elementos de la clase \( i \) en el nodo \( S \).

### 2. **Divisi√≥n Recursiva**

En cada nodo, el algoritmo de √°rbol de decisi√≥n selecciona la caracter√≠stica que mejor divide los datos, bas√°ndose en los criterios de impureza o ganancia de informaci√≥n. La divisi√≥n es **recursiva** hasta que se cumple alguna condici√≥n de detenci√≥n, como:
- **Profundidad m√°xima del √°rbol**.
- **N√∫mero m√≠nimo de muestras en un nodo**.
- **Valor m√≠nimo de impureza**.

### 3. **Poda (Pruning)**

Para evitar **sobreajuste (overfitting)**, despu√©s de construir el √°rbol, se pueden **podar** algunas ramas. La poda elimina aquellas ramas que no contribuyen significativamente al poder predictivo del modelo.

---

## üß† ¬øQu√© Hace que un √Årbol de Decisi√≥n Sea √ötil?

1. **Intuici√≥n y Facilidad de Interpretaci√≥n**: Los √°rboles de decisi√≥n son f√°ciles de entender y de visualizar, lo que facilita la interpretaci√≥n de c√≥mo se toman las decisiones.

2. **No Requiere Preprocesamiento de los Datos**: A diferencia de otros modelos, los √°rboles de decisi√≥n no requieren que las caracter√≠sticas est√©n escaladas o transformadas.

3. **Capacidad para Manejar Relaciones No Lineales**: Pueden modelar relaciones no lineales entre las caracter√≠sticas y la variable objetivo.

4. **Clasificaci√≥n y Regresi√≥n**: Se usan tanto para problemas de clasificaci√≥n (cuando la variable objetivo es categ√≥rica) como de regresi√≥n (cuando la variable objetivo es continua).

---

## üöß Limitaciones de los √Årboles de Decisi√≥n

1. **Sobreajuste (Overfitting)**: Si el √°rbol es muy profundo, puede ajustarse demasiado a los datos de entrenamiento y perder capacidad de generalizaci√≥n.

2. **Inestabilidad**: Los √°rboles de decisi√≥n pueden ser inestables si los datos tienen peque√±as variaciones. Un peque√±o cambio en los datos puede producir un √°rbol completamente diferente.

3. **Sesgo hacia Atributos con Muchas Divisiones**: Si algunas caracter√≠sticas tienen muchas m√°s posibles divisiones que otras, el √°rbol puede volverse sesgado y tomar decisiones basadas en esas caracter√≠sticas sin importancia real.

---

## üõ† ¬øPara Qu√© Sirven los √Årboles de Decisi√≥n?

- **Clasificaci√≥n**: Se usan para clasificar datos en categor√≠as. Ejemplo t√≠pico: clasificaci√≥n de correos electr√≥nicos como "spam" o "no spam".
  
- **Regresi√≥n**: Para predecir valores continuos, como el precio de una casa basado en caracter√≠sticas como el tama√±o, la ubicaci√≥n y el n√∫mero de habitaciones.

- **An√°lisis Exploratorio de Datos**: Ayudan a explorar los datos y a entender qu√© variables son las m√°s relevantes para predecir una determinada clase.

---

## üîë Resumen

Un **√Årbol de Decisi√≥n** es un modelo de **aprendizaje supervisado** que divide los datos en grupos homog√©neos bas√°ndose en reglas sobre las caracter√≠sticas. Es f√°cil de interpretar y se usa tanto para **clasificaci√≥n** como para **regresi√≥n**. Aunque poderosos, tienen limitaciones como el **sobreajuste**, pero esto se puede mitigar con t√©cnicas como la **poda** y el uso de **Random Forests**.
![image](https://github.com/user-attachments/assets/5d555fc7-d7fa-4756-bfc4-72fdad7c4c5b)

