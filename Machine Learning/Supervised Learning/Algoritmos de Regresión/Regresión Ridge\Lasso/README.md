# ğŸ§· Ridge y Lasso Regression en Machine Learning

La regresiÃ³n lineal tradicional puede sufrir **sobreajuste (overfitting)** cuando el modelo es muy complejo, hay demasiadas variables o los datos tienen ruido. Las tÃ©cnicas de **Ridge** y **Lasso Regression** introducen **regularizaciÃ³n**, que ayuda a controlar la complejidad del modelo penalizando los coeficientes grandes.

---

## ğŸ¯ Â¿QuÃ© hacen?

Ambas tÃ©cnicas modifican el entrenamiento de un modelo de regresiÃ³n lineal agregando una penalizaciÃ³n al tamaÃ±o de los coeficientes:

- **Ridge Regression**: reduce el tamaÃ±o de todos los coeficientes, sin llegar a eliminarlos completamente.
- **Lasso Regression**: puede forzar algunos coeficientes a exactamente cero, eliminando variables del modelo (selecciÃ³n de caracterÃ­sticas).

---
##  âœ… Ventajas
TÃ©cnica	Ventajas principales
Ridge	- Funciona bien con muchas variables correlacionadas
- Reduce la complejidad sin eliminar variables
Lasso	- Elimina variables irrelevantes automÃ¡ticamente
- Mejora interpretabilidad al generar modelos mÃ¡s simples
---

##  ğŸ§ª Casos de Uso
Escenario	TÃ©cnica recomendada
Muchas variables correlacionadas	Ridge
ReducciÃ³n automÃ¡tica de caracterÃ­sticas	Lasso
PrevenciÃ³n de sobreajuste	Ambos
---

##  âš ï¸ Limitaciones
Ridge no realiza selecciÃ³n de variables: incluye todas las caracterÃ­sticas.

Lasso puede comportarse de forma inestable si las variables estÃ¡n altamente correlacionadas.

El valor de alpha debe seleccionarse cuidadosamente mediante validaciÃ³n cruzada.

Ambos pueden tener bajo rendimiento si no se ajusta adecuadamente la regularizaciÃ³n.
---

##  ğŸ“Š ComparaciÃ³n rÃ¡pida
CaracterÃ­stica	Ridge Regression	Lasso Regression
PenalizaciÃ³n	L2	L1
Reduce coeficientes	SÃ­	SÃ­
Coeficientes en cero	No	SÃ­ (puede eliminar variables)
SelecciÃ³n de variables	No	SÃ­
Interpretabilidad	Media	Alta
Multicolinealidad	Bien manejada	Puede ser un problema
---

##  ğŸ“ˆ VisualizaciÃ³n


ComparaciÃ³n geomÃ©trica entre la penalizaciÃ³n L1 (Lasso) y L2 (Ridge)

