# ğŸ”— Elastic Net Regression

## ğŸ¤” Â¿QuÃ© es Elastic Net?

Elastic Net es una tÃ©cnica de regresiÃ³n lineal regularizada que combina las penalizaciones de **Ridge (L2)** y **Lasso (L1)** en un solo modelo. Su objetivo es obtener lo mejor de ambos mÃ©todos: regularizaciÃ³n que reduzca la complejidad del modelo y selecciÃ³n automÃ¡tica de variables.

## âš™ï¸ Â¿CÃ³mo funciona?

Elastic Net agrega a la funciÃ³n de pÃ©rdida de la regresiÃ³n lineal dos tÃ©rminos de penalizaciÃ³n simultÃ¡neamente:

- **L1 (Lasso):** promueve la sparsidad, es decir, hace que algunos coeficientes se vuelvan exactamente cero, facilitando la selecciÃ³n de variables.
- **L2 (Ridge):** reduce el tamaÃ±o de los coeficientes, ayudando a manejar multicolinealidad y evitando coeficientes extremadamente grandes.

La funciÃ³n objetivo que minimiza Elastic Net es:

$$
J(\beta) = \frac{1}{2n} \sum_{i=1}^n \left( y_i - \hat{y}_i \right)^2 + \alpha \left( \lambda \|\beta\|_1 + \frac{1-\lambda}{2} \|\beta\|_2^2 \right)
$$

donde:

- \( \alpha \) controla la fuerza total de la regularizaciÃ³n,
- \( \lambda \in [0,1] \) es el parÃ¡metro `l1_ratio` que define la mezcla entre L1 y L2,
- \( \|\beta\|_1 \) es la norma L1 (suma de valores absolutos),
- \( \|\beta\|_2^2 \) es la norma L2 al cuadrado.

## ğŸ§© Â¿En quÃ© casos se usa?

Elastic Net es especialmente Ãºtil cuando:

- ğŸ“Š Hay **muchas variables** (alta dimensionalidad).
- ğŸ”— Las variables estÃ¡n **correlacionadas** entre sÃ­ (multicolinealidad).
- ğŸ—‘ï¸ Se desea hacer **selecciÃ³n de variables**, pero sin perder la estabilidad que ofrece Ridge.
- âš–ï¸ Se quiere un compromiso entre la **sparsidad** de Lasso y la **estabilidad** de Ridge.

## âœ¨ Ventajas respecto a otros algoritmos de regresiÃ³n

- Combina lo mejor de Ridge y Lasso, permitiendo un balance ajustable entre regularizaciÃ³n y selecciÃ³n de variables.
- MÃ¡s robusto que Lasso en presencia de variables altamente correlacionadas.
- Puede seleccionar grupos de variables correlacionadas en lugar de elegir arbitrariamente solo una.
- Flexibilidad para adaptarse mejor a distintos tipos de problemas mediante ajuste de hiperparÃ¡metros.

## âš ï¸ Limitaciones

- Requiere optimizaciÃ³n de dos hiperparÃ¡metros (`alpha` y `l1_ratio`), lo que puede aumentar el costo computacional.
- No siempre es trivial interpretar el efecto individual de las variables cuando se combinan penalizaciones.
- Si no se ajustan bien los hiperparÃ¡metros, puede no superar en desempeÃ±o a Ridge o Lasso por separado.
- No es adecuado si la relaciÃ³n entre variables y objetivo es altamente no lineal sin transformar los datos primero.

