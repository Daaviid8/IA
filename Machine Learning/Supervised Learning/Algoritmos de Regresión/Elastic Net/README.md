# ğŸ”— Elastic Net Regression

## ğŸ¤” Â¿QuÃ© es Elastic Net?

Elastic Net es una tÃ©cnica de regresiÃ³n lineal regularizada que combina las penalizaciones de **Ridge (L2)** y **Lasso (L1)** en un solo modelo. Su objetivo es obtener lo mejor de ambos mÃ©todos: regularizaciÃ³n que reduzca la complejidad del modelo y selecciÃ³n automÃ¡tica de variables.

## âš™ï¸ Â¿CÃ³mo funciona?

Elastic Net agrega a la funciÃ³n de pÃ©rdida de la regresiÃ³n lineal dos tÃ©rminos de penalizaciÃ³n simultÃ¡neamente:

- **L1 (Lasso):** promueve la sparsidad, es decir, hace que algunos coeficientes se vuelvan exactamente cero, facilitando la selecciÃ³n de variables.
- **L2 (Ridge):** reduce el tamaÃ±o de los coeficientes, ayudando a manejar multicolinealidad y evitando coeficientes extremadamente grandes.

La funciÃ³n objetivo que minimiza Elastic Net es:

$$
J(\beta) = \frac{1}{2n} \sum_{i=1}^n \left( y_i - \hat{y}_i \right)^2 + \alpha \left( \lambda \|\beta\|_1 + \frac{1-\lambda}{2} \|\beta\|_2^2 \right)
$$

donde:

Donde:

- \( J(\beta) \) es la **funciÃ³n objetivo** que queremos minimizar para encontrar los mejores coeficientes \(\beta\).
- \( n \) es el nÃºmero total de muestras en el dataset.
- \( y_i \) es el valor real de la variable objetivo para la muestra \(i\).
- \( \hat{y}_i \) es el valor predicho por el modelo para la muestra \(i\).
- \( \beta \) es el vector de coeficientes del modelo (los parÃ¡metros que queremos aprender).
- \( \|\beta\|_1 = \sum_{j} |\beta_j| \) es la norma L1 (suma de valores absolutos de los coeficientes).
- \( \|\beta\|_2^2 = \sum_{j} \beta_j^2 \) es la norma L2 al cuadrado (suma de los cuadrados de los coeficientes).
- \( \alpha \) controla la fuerza total de la regularizaciÃ³n (quÃ© tanto penalizamos la complejidad del modelo).
- \( \lambda \) (o `l1_ratio`) controla la proporciÃ³n entre penalizaciÃ³n L1 y L2:
  - Cuando \( \lambda = 1 \), Elastic Net es igual a Lasso (solo L1).
  - Cuando \( \lambda = 0 \), Elastic Net es igual a Ridge (solo L2).

## ğŸ§© Â¿En quÃ© casos se usa?

Elastic Net es especialmente Ãºtil cuando:

- ğŸ“Š Hay **muchas variables** (alta dimensionalidad).
- ğŸ”— Las variables estÃ¡n **correlacionadas** entre sÃ­ (multicolinealidad).
- ğŸ—‘ï¸ Se desea hacer **selecciÃ³n de variables**, pero sin perder la estabilidad que ofrece Ridge.
- âš–ï¸ Se quiere un compromiso entre la **sparsidad** de Lasso y la **estabilidad** de Ridge.

## âœ¨ Ventajas respecto a otros algoritmos de regresiÃ³n

- Combina lo mejor de Ridge y Lasso, permitiendo un balance ajustable entre regularizaciÃ³n y selecciÃ³n de variables.
- MÃ¡s robusto que Lasso en presencia de variables altamente correlacionadas.
- Puede seleccionar grupos de variables correlacionadas en lugar de elegir arbitrariamente solo una.
- Flexibilidad para adaptarse mejor a distintos tipos de problemas mediante ajuste de hiperparÃ¡metros.

## âš ï¸ Limitaciones

- Requiere optimizaciÃ³n de dos hiperparÃ¡metros (`alpha` y `l1_ratio`), lo que puede aumentar el costo computacional.
- No siempre es trivial interpretar el efecto individual de las variables cuando se combinan penalizaciones.
- Si no se ajustan bien los hiperparÃ¡metros, puede no superar en desempeÃ±o a Ridge o Lasso por separado.
- No es adecuado si la relaciÃ³n entre variables y objetivo es altamente no lineal sin transformar los datos primero.

